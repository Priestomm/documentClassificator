{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "We import the packages that include corpora, tokenizer, lemmatizer and stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import collections\n",
    "import nltk\n",
    "import string\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import EuroparlCorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stopwords, punctuations and the first 10000 number in order to remove them from the BOW\n",
    "\n",
    "We remove them because they don't have any informative meaning for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['english', 'italian', 'spanish', 'dutch', 'german', 'portuguese', 'finnish', 'swedish', 'greek']\n",
    "numbers = []\n",
    "for number in range(10000):\n",
    "    numbers.append(str(number))\n",
    "stop_words = list(nltk.corpus.stopwords.words(lang) for lang in languages)\n",
    "punctuation = string.punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the corpora we're using to train and to test our classifier\n",
    "\n",
    "In particular, I import corpora from nltk.corpus.EuroParl, a collection that contains extracts from public speeches at the european parliament, in different languages.\n",
    "\n",
    "Each corpus is divided into paragraphs, split by the character '\\n', and each paragraph is labeled as 'eng' or 'noteng'.\n",
    "\n",
    "Then I shuffle all the documents I stored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19990\n",
      "29109\n"
     ]
    }
   ],
   "source": [
    "# Be sure you set the right directory\n",
    "\n",
    "root = 'data/europarl_raw/'\n",
    "\n",
    "english = EuroparlCorpusReader(root, '.*\\.en')\n",
    "documents = list()\n",
    "for fileid in english.fileids():\n",
    "    for sentence in english.raw(fileid).split('\\n'):\n",
    "        documents.append((sentence, 'eng'))\n",
    "\n",
    "prefToLang = {'da':'danish', 'nl':'dutch', 'fi':'finnish', 'de':'german', 'fr':'french', 'it':'italian', 'pt':'portuguese', 'el':'greek', 'es':'spanish', 'sv':'swedish'}\n",
    "\n",
    "for lang in prefToLang.keys():\n",
    "    non_english = EuroparlCorpusReader(root, \".*\\.{}\".format(lang))\n",
    "    for sentence in non_english.raw(f'{prefToLang[lang]}/ep-00-01-17.{lang}').split('\\n'):\n",
    "        documents.append((sentence, 'noteng'))\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import the stemmer and the lemmatizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "I tokenize the paragraphs stored into 'documents'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list()\n",
    "for document in documents:\n",
    "    for word in word_tokenize(document[0]):\n",
    "        words.append(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming, lemmatizating and removing stopwords, punctuation and numbers from the words we obtained by tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_without_sw = []\n",
    "\n",
    "for word in words:\n",
    "    if word not in stop_words and word not in punctuation and word not in numbers:\n",
    "        # print(f\"{word} : {lemmatizer.lemmatize(word)} : {stemmer.stem(lemmatizer.lemmatize(word))}\")\n",
    "        words_without_sw.append(lemmatizer.lemmatize(stemmer.stem(word)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bag of Word\n",
    "We compute the probability of the words in our corpora, then store the first most frequent 5000 in a list."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = nltk.probability.FreqDist(words_without_sw)# BOW\n",
    "word_feature = list(freq)[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# We define the function that will extract out features\n",
    "For each document, the function returns a dictionary that verifies which words, from the document we pass in input, are contained into the BOW."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for word in word_feature:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the featureset\n",
    "A list of tuples, that associates the label 'eng' or 'noteng' to each paragraph of the corpora we imported previously."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# I initiate the Naive Bayes Classifier\n",
    "- Train set: first half of the featureset\n",
    "- Test set: second half of the featureset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29109\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_set, test_set = featuresets[math.floor(len(featuresets)/2):], featuresets[:math.floor(len(featuresets)/2)]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing the classifier on new sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noteng\n"
     ]
    }
   ],
   "source": [
    "print(classifier.classify(document_features(open('prova.txt').read())))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "refMat = list()\n",
    "testMat = list()\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refMat.append('eng') if label == 'eng' else refMat.append('noteng')\n",
    "    observed = classifier.classify(feats)\n",
    "    testMat.append('eng') if observed == 'eng' else testMat.append('noteng')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Display stats\n",
    "\n",
    "- **Precision**: The number of true positives / true positives + false positives\n",
    "- **Recall**: The number of trie positives / true positives + false negatives\n",
    "- **F-Measure**: 2 * Precision * Recall / Precision + Recall\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tag | Prec.  | Recall | F-measure\n",
      "-------+--------+--------+-----------\n",
      "   eng | 1.0000 | 0.9919 | 0.9959\n",
      "noteng | 0.9824 | 1.0000 | 0.9911\n",
      "\n",
      "Precision: 1.0\n",
      "Recall: 0.9919266420811322\n",
      "F-Measure: 0.995946960220165\n"
     ]
    }
   ],
   "source": [
    "cm = ConfusionMatrix(refMat, testMat)\n",
    "print(cm.evaluate())"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
